{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Driver Cell for the whole program \n",
    "\n",
    "# To run this notebook , you need to have the directory = \"output\" in the same folder where this folder is executed \n",
    "\n",
    "import time\n",
    "args = {}\n",
    "args['Comments'] = \"N\"\n",
    "args['axis'] = 2\n",
    "args['binary'] = False\n",
    "args['components'] = 2\n",
    "args['decom'] = 'parafac'\n",
    "args['directory'] = 'output'\n",
    "args['file'] = None\n",
    "args['heatmap'] = False\n",
    "args['kmeans']= True\n",
    "args['lines'] = 100\n",
    "args['ngrams'] = 1\n",
    "args['text'] = True\n",
    "args['output_option'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initilizing the Tensor\n",
      "Creating a TermDocumentTensor\n",
      "Document TermDocumentTensor\n",
      "Finished tensor construction.\n",
      "Tensor shape:(9, 2680, 166)\n",
      "  4.14 seconds for TDM document\n",
      "In decomposition of a TDM\n",
      "  4.11 seconds for decomposition of a tensor\n",
      "Generating a cosine similarity matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/scipy/spatial/distance.py:505: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  13.54 seconds is the total time for program to execute\n"
     ]
    }
   ],
   "source": [
    "#Main function which will take the inputs from the previous cell and create the tensor and its decompositions\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    file_type = \"binary\" if args['binary'] else \"text\"\n",
    "    tdt =TermDocumentTensor(args['directory'], type=file_type, file_name=args['file'])\n",
    "    tdt.create_term_document_tensor(ngrams=args['ngrams'], lines=args['lines'])\n",
    "    if args['decom'] == \"parafac\":\n",
    "        factors = tdt.parafac_decomposition()\n",
    "    visualize = TensorVisualization()\n",
    "    if args['heatmap']:\n",
    "        cos_sim = tdt.generate_cosine_similarity_matrix(factors[args['axis']])\n",
    "        visualize.generate_heat_map(cos_sim, tdt.corpus_names)\n",
    "    if args['kmeans']:\n",
    "        visualize.k_means_clustering(factors[args['axis']], tdt.corpus_names, clusters=args['components'])\n",
    "        tdt.generate_cosine_similarity_matrix(factors[args['axis']])\n",
    "    print(\"  %s seconds is the total time for program to execute\" % format((time.time() - start_time), '.2f'))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defining the class termdocumenttensor which has the functions like creating the tensor,decomposing the tensor,\n",
    "#generation of the similiarity matrix and its clustering \n",
    "\n",
    "flag = 1\n",
    "from scipy import spatial\n",
    "from collections import deque\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import _pickle as pickle\n",
    "import tensorflow as tf\n",
    "from ktensor import KruskalTensor\n",
    "import time\n",
    "\n",
    "def flag_function_tdm(cmts):\n",
    "        global flag\n",
    "        flag = cmts\n",
    "\n",
    "class TermDocumentTensor():\n",
    "    def __init__(self, directory, type=\"binary\", file_name=None):\n",
    "        if flag==1:\n",
    "            print(\"Initilizing the Tensor\")\n",
    "        self.vocab = []\n",
    "        self.tensor = []\n",
    "        self.corpus_names = []\n",
    "        self.directory = directory\n",
    "        self.type = type\n",
    "        self.rank_approximation = None\n",
    "        self.factor_matrices = []\n",
    "        # These are the output of our tensor decomposition.\n",
    "        self.factors = []\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def generate_cosine_similarity_matrix(self, matrix):\n",
    "        f = open('cosine.txt', 'w');\n",
    "        if flag == 1:\n",
    "            print(\"Generating a cosine similarity matrix\")\n",
    "        cosine_sim = []\n",
    "        for entry in matrix:\n",
    "            sim = []\n",
    "            for other_entry in matrix:\n",
    "                sim.append(spatial.distance.cosine(entry, other_entry) * -1 + 1)\n",
    "                f.write(str(spatial.distance.cosine(entry, other_entry) * -1 + 1))\n",
    "                f.write(\"\\n\")\n",
    "            cosine_sim.append(sim)\n",
    "        return cosine_sim\n",
    "\n",
    "    def get_estimated_rank(self):\n",
    "        \"\"\"\n",
    "        Getting the rank of a tensor is an NP hard problem\n",
    "        Therefore we use an estimation based on the size of the dimensions of our tensor.\n",
    "        These numbers are grabbed from Table 3.3 of Tammy Kolda's paper:\n",
    "        http://www.sandia.gov/~tgkolda/pubs/pubfiles/TensorReview.pdf\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # At the moment the rank returned by this function is normally too high for either\n",
    "        # my machine or the tensorly library to handle, therefore I have made it just return 1 for right now\n",
    "        if flag == 1:\n",
    "            print(\"Estimation of the rank of tensor \")\n",
    "        I = len(self.tensor[0])\n",
    "        J = len(self.tensor[0][0])\n",
    "        K = len(self.tensor)\n",
    "\n",
    "        if I == 1 or J == 1 or K == 1:\n",
    "            return 1\n",
    "        elif I == J == K == 2:\n",
    "            return 2\n",
    "        elif I == J == 3 and K == 2:\n",
    "            return 3\n",
    "        elif I == 5 and J == K == 3:\n",
    "            return 5\n",
    "        elif I >= 2 * J and K == 2:\n",
    "            return 2 * J\n",
    "        elif 2 * J > I > J and K == 2:\n",
    "            return I\n",
    "        elif I == J and K == 2:\n",
    "            return I\n",
    "        elif I >= J * K:\n",
    "            return J * K\n",
    "        elif J * K - J < I < J * K:\n",
    "            return I\n",
    "        elif I == J * K - I:\n",
    "            return I\n",
    "        else:\n",
    "            print(I, J, K, \"did not have an exact estimation\")\n",
    "            return min(I * J, I * K, J * K)\n",
    "\n",
    "    def print_formatted_term_document_tensor(self):\n",
    "        if flag == 1:\n",
    "            print(\"Print the TDM\")\n",
    "        for matrix in self.tensor:\n",
    "            print(self.vocab)\n",
    "            for i in range(len(matrix)):\n",
    "                print(self.corpus_names[i], matrix[i])\n",
    "\n",
    "    def create_term_document_tensor(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Generic tensor creation function. Returns different tensor based on user input.\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if flag == 1:\n",
    "            print(\"Creating a TermDocumentTensor\")\n",
    "        if self.type == \"binary\":\n",
    "            return self.create_binary_term_document_tensor(**kwargs)\n",
    "        else:\n",
    "            return self.create_term_document_tensor_text(**kwargs)\n",
    "\n",
    "    def create_binary_term_document_tensor(self, **kwargs):\n",
    "        start_time1 = time.time()\n",
    "        if flag == 1:\n",
    "            print(\"Binary TermDocumentTensor\")\n",
    "        doc_content = []\n",
    "        first_occurences_corpus = {}\n",
    "        ngrams = kwargs[\"ngrams\"] if kwargs[\"ngrams\"] is not None else 1\n",
    "        print(ngrams)\n",
    "\n",
    "        for file_name in os.listdir(self.directory):\n",
    "            previous_bytes = deque()\n",
    "            first_occurences = {}\n",
    "            byte_count = 0\n",
    "            with open(self.directory + \"/\" + file_name, \"rb\") as file:\n",
    "                my_string = \"\"\n",
    "                while True:\n",
    "                    byte_count += 1\n",
    "                    current_byte = file.read(1).hex()\n",
    "                    if not current_byte:\n",
    "                        break\n",
    "                    if byte_count >= ngrams:\n",
    "                        byte_gram = \"\".join(list(previous_bytes)) + current_byte\n",
    "                        if byte_gram not in first_occurences:\n",
    "                            first_occurences[byte_gram] = byte_count\n",
    "                        if byte_count % ngrams == 0:\n",
    "                            my_string += byte_gram + \" \"\n",
    "                        if ngrams > 1:\n",
    "                            previous_bytes.popleft()\n",
    "                    if ngrams > 1:\n",
    "                        previous_bytes.append(current_byte)\n",
    "                first_occurences_corpus[file_name] = first_occurences\n",
    "            doc_content.append(my_string)\n",
    "        doc_names = os.listdir(self.directory)\n",
    "\n",
    "        # Convert a collection of text documents to a matrix of token counts\n",
    "        vectorizer = TfidfVectorizer(use_idf=False)\n",
    "        # Learn the vocabulary dictionary and return term-document matrix.\n",
    "        x1 = vectorizer.fit_transform(doc_content).toarray()\n",
    "        del doc_content\n",
    "        self.vocab = [\"vocab\"]\n",
    "\n",
    "        self.vocab.extend(vectorizer.get_feature_names())\n",
    "        tdm = []\n",
    "        for i in range(len(doc_names)):\n",
    "            row = x1[i]\n",
    "            tdm.append(row)\n",
    "        svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)\n",
    "        reduced_tdm = svd.fit_transform(tdm)\n",
    "        tdm_first_occurences = []\n",
    "        self.corpus_names = doc_names\n",
    "        # Create a first occurences matrix that corresponds with the tdm\n",
    "        for j in range(len(doc_names)):\n",
    "            item = doc_names[j]\n",
    "            this_tdm = []\n",
    "            for i in range(0, len(tdm[0])):\n",
    "                word = self.vocab[i]\n",
    "                try:\n",
    "                    this_tdm.append(first_occurences_corpus[item][word])\n",
    "                except:\n",
    "                    this_tdm.append(0)\n",
    "            # print(this_tdm)\n",
    "            tdm_first_occurences.append(this_tdm)\n",
    "        reduced_tdm_first_occurences = svd.fit_transform(tdm_first_occurences)\n",
    "        del tdm_first_occurences\n",
    "        del tdm\n",
    "        tdt = [reduced_tdm, reduced_tdm_first_occurences]\n",
    "        self.tensor = tdt\n",
    "        #tdm_sparse = scipy.sparse.csr_matrix(tdm)\n",
    "        #tdm_first_occurences_sparse = scipy.sparse.csr_matrix(tdm_first_occurences)\n",
    "        if flag == 1:\n",
    "            print(\"  %s seconds for TDM Binary\" % format((time.time() - start_time1), '.2f'))\n",
    "        return self.tensor\n",
    "\n",
    "    def create_term_document_tensor_text(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates term-sentence-document tensor out of files in directory\n",
    "        Attempts to save this tensor to a pickle file\n",
    "        \n",
    "        :return: 3-D dense numpy array, self.tensor\n",
    "        \"\"\"\n",
    "        start_time2 = time.time()\n",
    "        if flag == 1:\n",
    "            print(\"Document TermDocumentTensor\")\n",
    "\n",
    "        self.tensor = None\n",
    "        vectorizer = TfidfVectorizer(use_idf=False, analyzer=\"word\")\n",
    "        document_cutoff_positions = []\n",
    "        doc_content = []\n",
    "        pos = 0\n",
    "        max_matrix_height = 0\n",
    "        max_sentences = kwargs[\"lines\"]\n",
    "        self.corpus_names = os.listdir(self.directory)\n",
    "\n",
    "        # If given Pickle file, read it in\n",
    "        if self.file_name is not None:\n",
    "            file = open(self.file_name, 'rb')\n",
    "            self.tensor = pickle.load(file)\n",
    "            return self.tensor\n",
    "\n",
    "        # Create one large term document matrix from all documents. Done to ensure same vocabulary.\n",
    "        for file_name in self.corpus_names:\n",
    "            document_cutoff_positions.append(pos)\n",
    "            with open(self.directory + \"/\" + file_name, \"r\", errors=\"ignore\") as file:\n",
    "                for line in file:\n",
    "                    if len(line) > 2:\n",
    "                        pos += 1\n",
    "                        doc_content.append(line)\n",
    "                    if pos - document_cutoff_positions[-1] >= max_sentences:\n",
    "                        break\n",
    "                if max_matrix_height < pos - document_cutoff_positions[-1]:\n",
    "                    max_matrix_height = pos - document_cutoff_positions[-1]\n",
    "\n",
    "        document_cutoff_positions.append(pos)\n",
    "\n",
    "        x1 = vectorizer.fit_transform(doc_content)\n",
    "        matrix_length = len(vectorizer.get_feature_names())\n",
    "\n",
    "        # Split large term document matrix, into term document tensor. Splits happen where one document ends.\n",
    "        for i in range(len(document_cutoff_positions) - 1):\n",
    "            temp = x1[document_cutoff_positions[i]:document_cutoff_positions[i + 1], :]\n",
    "            temp = temp.todense()\n",
    "            # Make all matrix slices the same size\n",
    "            term_sentence_matrix = np.zeros((max_matrix_height, matrix_length))\n",
    "            term_sentence_matrix[:temp.shape[0], :temp.shape[1]] = temp\n",
    "            if self.tensor is None:\n",
    "                self.tensor = term_sentence_matrix\n",
    "            else:\n",
    "                self.tensor = np.dstack((self.tensor, term_sentence_matrix))\n",
    "\n",
    "        self.file_name = self.directory + \".pkl\"\n",
    "        if flag == 1:\n",
    "            print(\"Finished tensor construction.\")\n",
    "        if flag == 1:\n",
    "            print(\"Tensor shape:\" + str(self.tensor.shape))\n",
    "        try:\n",
    "            pickle.dump(self.tensor, open(self.file_name, \"wb\"))\n",
    "        except OverflowError:\n",
    "            print(\"ERROR: Tensor cannot be saved to pickle file due to size larger than 4 GiB\")\n",
    "        if flag == 1:\n",
    "            print(\"  %s seconds for TDM document\" % format((time.time() - start_time2),'.2f'))\n",
    "        return self.tensor\n",
    "\n",
    "    def parafac_decomposition(self):\n",
    "        \"\"\"\n",
    "        Computes a parafac decomposition of the tensor.\n",
    "        This will return n rank 3 factor matrices. Where n represents the dimensionality of the tensor.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        start_time3 = time.time()\n",
    "        if flag == 1:\n",
    "            print(\"In decomposition of a TDM\")\n",
    "        decompose = KruskalTensor(self.tensor.shape, rank=3, regularize=1e-6, init='nvecs', X_data=self.tensor)\n",
    "        self.factors = decompose.U\n",
    "        with tf.Session() as sess:\n",
    "            for i in range(len(self.factors)):\n",
    "                sess.run(self.factors[i].initializer)\n",
    "                self.factors[i] = self.factors[i].eval()\n",
    "        if flag == 1:\n",
    "            print(\"  %s seconds for decomposition of a tensor\" % format((time.time() - start_time3), '.2f'))\n",
    "        return self.factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tensorvisualization class which takes the similarity matrix and vizualizes them into Clusters or Heatmaps\n",
    "flag =1\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import time\n",
    "import scipy.cluster.hierarchy\n",
    "import scipy.spatial.distance\n",
    "from collections import Counter\n",
    "import accuracy\n",
    "\n",
    "\n",
    "def flag_function_visualization(cmts):\n",
    "    global flag\n",
    "    flag = cmts\n",
    "\n",
    "\n",
    "class TensorVisualization():\n",
    "    def __init__(self):\n",
    "        plotly.tools.set_credentials_file(username='MaxPoole', api_key='2ajqCLZjiLNDFxgyLtGn')\n",
    "\n",
    "    def generate_heat_map(self, data, axis_labels):\n",
    "        \"\"\"\n",
    "        Generates a heat map for the current data\n",
    "        Currently only meant to support using a cosine similarity matrix\n",
    "        :param data:\n",
    "        :param axis_labels:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if flag == 1:\n",
    "            print(\"Generating Heatmap \")\n",
    "        axis_labels_abbreviated = [label[:14] for label in axis_labels]\n",
    "        info = [go.Heatmap(z=data,\n",
    "                           x=axis_labels_abbreviated,\n",
    "                           y=axis_labels_abbreviated,\n",
    "                           colorscale='Hot',\n",
    "                           )]\n",
    "\n",
    "        layout = go.Layout(title='Cosine Similarity Between Documents',\n",
    "                           xaxis=dict(ticks=' '),\n",
    "                           yaxis=dict(ticks=' '),\n",
    "                           plot_bgcolor='#444',\n",
    "                           paper_bgcolor='#eee'\n",
    "                           )\n",
    "        fig = go.Figure(data=info, layout=layout)\n",
    "        plotly.offline.plot(fig, filename='malware_heatmap.html')\n",
    "\n",
    "    def k_means_clustering(self, factor_matrix, file_names=[], clusters=2):\n",
    "        clusters = 2\n",
    "        svd = TruncatedSVD(n_components=2, n_iter=20, random_state=42)\n",
    "        reduced = svd.fit_transform(factor_matrix)\n",
    "        kmeans = KMeans(n_clusters=clusters, random_state=0).fit(factor_matrix)\n",
    "        labels_predicted = kmeans.labels_\n",
    "        data = [plotly.graph_objs.Scatter(x=[entry[0] for entry in reduced],\n",
    "                                          y=[entry[1] for entry in reduced],\n",
    "                                          mode='markers',\n",
    "                                          marker=dict(color=kmeans.labels_),\n",
    "                                          text=file_names\n",
    "                                          )\n",
    "                ]\n",
    "        fig = go.Figure(data=data)\n",
    "        plotly.offline.plot(fig, filename='kmeans_cluster.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
