{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Decomposition for Malware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have code like this at the top of every notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'whereAmI'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f625efd337b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#from LZJD import *         # do'nt seem to need LZJD in this notebook, yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwhereAmI\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetcwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'whereAmI'"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "\n",
    "#from LZJD import *         # do'nt seem to need LZJD in this notebook, yet\n",
    "from whereAmI import *\n",
    "from os import chdir\n",
    "from os import getcwd\n",
    "from os import listdir\n",
    "from os import path\n",
    "\n",
    "(home, dropBoxDir, dataDir) = whereAmI()\n",
    "\n",
    "print(dropBoxDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing TensorFlow\n",
    "\n",
    "We need TensorFlow to do the machine learning work in this notebook.  To install tensorflow on a Ubuntu guest, try this:\n",
    "\n",
    "which pip     # to find out which version of pip to use\n",
    "\n",
    "/home/charles/anaconda3/bin/pip install -U tensorflow  # may need full path if using sudo\n",
    "\n",
    "But contrary to the documentation at \n",
    "https://www.tensorflow.org/install/install_linux#InstallingNativePip,\n",
    "it doesn't seem that sudo is necessary.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if tensorflow is installed, try the following as a stand-alone script.  The cell shown below is markup, and is not intended for execution inside this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`python -c \"import tensorflow as tf; print(tf.__version__)\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that works, then the following cell should also work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing tensorflow the first time, there was this warning:\n",
    "    \n",
    "`/home/charles/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: \n",
    "FutureWarning: Conversion of the second argument of issubdtype from \n",
    "\"float\" to \"np.floating\" is deprecated. In future, it will be treated \n",
    "as \"np.float64 == np.dtype(float).type\".  from ._conv import register_converters as _register_converters`\n",
    "\n",
    "so after a couple of Google searches, upgrading to a newer version of h5py seems to make the warning go away.\n",
    "\n",
    "`pip install h5py==2.8.0rc1`\n",
    "\n",
    "`python -c 'import h5py; print(h5py.version.info)'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some options for the program..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to have some tensor decomposition code that does not come with tensorflow, but uses it.  See https://github.com/ebigelow/tf-decompose.  The pip program doesn't seem to know about ktensor and its friends, so unzip the code from GitHub and copy ktensor.py, dtensor.py, and utils.py to the Jupyter process's directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor decomposition code uses tqdm, which looks handy anyway, and is easily installed:\n",
    "    `pip install tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ktensor import KruskalTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can parse the Shakespeare corpus, which I have in ~/Dropbox/working/vx/WS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining the class termdocumenttensor which has the functions like creating the tensor,\n",
    "# decomposing the tensor, generation of the similiarity matrix and its clustering \n",
    "\n",
    "from scipy import spatial\n",
    "from collections import deque\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import _pickle as pickle\n",
    "import time\n",
    "\n",
    "flag = 1   # print comments in a few places\n",
    "\n",
    "def flag_function_tdm(cmts):\n",
    "        global flag\n",
    "        flag = cmts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code needs plotly.  That's easy to install: `pip install plotly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tensorvisualization class which takes the similarity matrix and vizualizes \n",
    "# them into Clusters or Heatmaps\n",
    "flag =1\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import time\n",
    "import scipy.cluster.hierarchy\n",
    "import scipy.spatial.distance\n",
    "from collections import Counter\n",
    "#import accuracy    # Phani, is this package important?\n",
    "\n",
    "def flag_function_visualization(cmts):\n",
    "    global flag\n",
    "    flag = cmts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be several of these classes that end in \"mixin\".  By having no data in those classes, only methods, often just one method, they can be inherited by another class.  That allows us to have methods in their own cells in this notebook, allowing markup cells between them as documentation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class generate_heat_map_mixin:\n",
    "    def generate_heat_map(self, data, axis_labels):\n",
    "        \"\"\"\n",
    "        Generates a heat map for the current data\n",
    "        Currently only meant to support using a cosine similarity matrix\n",
    "        :param data:\n",
    "        :param axis_labels:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if flag == 1:\n",
    "            print(\"Generating heatmap...\")\n",
    "        axis_labels_abbreviated = [label[:14] for label in axis_labels]\n",
    "        info = [go.Heatmap(z=data,\n",
    "                           x=axis_labels_abbreviated,\n",
    "                           y=axis_labels_abbreviated,\n",
    "                           colorscale='Hot',\n",
    "                           )]\n",
    "\n",
    "        layout = go.Layout(title='Cosine Similarity Between Documents',\n",
    "                           xaxis=dict(ticks=''),\n",
    "                           yaxis=dict(ticks=''),\n",
    "                           plot_bgcolor='#444',\n",
    "                           paper_bgcolor='#eee'\n",
    "                           )\n",
    "        fig = go.Figure(data=info, layout=layout)\n",
    "        plotly.offline.plot(fig, filename='notebook_heatmap.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use SVD to do something, and then some clustering.  Not sure exactly what the underlying logic might be..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class k_mean_clustering_mixin:\n",
    "    def k_means_clustering(self, factor_matrix, file_names=[], clusters=2):\n",
    "        clusters = 2\n",
    "        svd = TruncatedSVD(n_components=2, n_iter=20, random_state=42)\n",
    "        reduced = svd.fit_transform(factor_matrix)\n",
    "        kmeans = KMeans(n_clusters=clusters, random_state=0).fit(factor_matrix)\n",
    "        labels_predicted = kmeans.labels_\n",
    "        data = [plotly.graph_objs.Scatter(x=[entry[0] for entry in reduced],\n",
    "                                          y=[entry[1] for entry in reduced],\n",
    "                                          mode='markers',\n",
    "                                          marker=dict(color=kmeans.labels_),\n",
    "                                          text=file_names\n",
    "                                          )\n",
    "                ]\n",
    "        fig = go.Figure(data=data)\n",
    "        plotly.offline.plot(fig, filename='kmeans_cluster.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TensorVisualization(generate_heat_map_mixin, k_mean_clustering_mixin):\n",
    "    def __init__(self):\n",
    "        from plotly import __version__\n",
    "        print( \"Using Plotly version \"+__version__) # requires version >= 1.9.0\n",
    "        plotly.tools.set_credentials_file(username='cknicholas', \n",
    "                                  api_key='pa9Z110GEeh029O4jTV0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class generate_cosine_similarity_mixin:\n",
    "    def generate_cosine_similarity_matrix(self, matrix):\n",
    "        f = open('cosine.txt', 'w');\n",
    "        if flag == 1:\n",
    "            print(\"Generating a cosine similarity matrix\")\n",
    "        cosine_sim = []\n",
    "        for entry in matrix:\n",
    "            sim = []\n",
    "            for other_entry in matrix:\n",
    "                sim.append(spatial.distance.cosine(entry, other_entry) * -1 + 1)\n",
    "                f.write(str(spatial.distance.cosine(entry, other_entry) * -1 + 1))\n",
    "                f.write(\"\\n\")\n",
    "            cosine_sim.append(sim)\n",
    "        return cosine_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This logic for estimating the rank of the tensor is interesting.  But I'm not sure if it's being used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class get_estimated_rank_mixin:\n",
    "    def get_estimated_rank(self):\n",
    "        \"\"\"\n",
    "        Getting the rank of a tensor is an NP hard problem\n",
    "        Therefore we use an estimation based on the size of the dimensions of our tensor.\n",
    "        These numbers are taken from Table 3.3 of Tammy Kolda's paper:\n",
    "        http://www.sandia.gov/~tgkolda/pubs/pubfiles/TensorReview.pdf\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # At the moment the rank returned by this function is normally too high for either\n",
    "        # my machine or the tensorly library to handle, therefore I have made it just \n",
    "        # return 1 for right now\n",
    "        if flag == 1:\n",
    "            print(\"Estimating the rank of the tensor...\")\n",
    "        I = len(self.tensor[0])\n",
    "        J = len(self.tensor[0][0])\n",
    "        K = len(self.tensor)\n",
    "\n",
    "        if I == 1 or J == 1 or K == 1:\n",
    "            return 1\n",
    "        elif I == J == K == 2:\n",
    "            return 2\n",
    "        elif I == J == 3 and K == 2:\n",
    "            return 3\n",
    "        elif I == 5 and J == K == 3:\n",
    "            return 5\n",
    "        elif I >= 2 * J and K == 2:\n",
    "            return 2 * J\n",
    "        elif 2 * J > I > J and K == 2:\n",
    "            return I\n",
    "        elif I == J and K == 2:\n",
    "            return I\n",
    "        elif I >= J * K:\n",
    "            return J * K\n",
    "        elif J * K - J < I < J * K:\n",
    "            return I\n",
    "        elif I == J * K - I:\n",
    "            return I\n",
    "        else:\n",
    "            print(I, J, K, \"did not have an exact estimation\")\n",
    "            return min(I * J, I * K, J * K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class create_term_document_tensor_mixin:\n",
    "    def create_term_document_tensor(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Generic tensor creation function. Returns different tensor based on user input.\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if flag == 1:\n",
    "            print(\"Creating a Term Document Tensor\")\n",
    "        if self.type == \"binary\":\n",
    "            return self.create_binary_term_document_tensor(**kwargs)\n",
    "        else:\n",
    "            return self.create_term_document_tensor_text(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class create_binary_term_document_tensor_mixin:\n",
    "    def create_binary_term_document_tensor(self, **kwargs):\n",
    "        start_time1 = time.time()\n",
    "        if flag == 1:\n",
    "            print(\"Binary Term Document Tensor\")\n",
    "        doc_content = []\n",
    "        first_occurences_corpus = {}\n",
    "        ngrams = kwargs[\"ngrams\"] if kwargs[\"ngrams\"] is not None else 1\n",
    "        print(\"ngrams is %s\" % (ngrams))\n",
    "\n",
    "        for file_name in os.listdir(self.directory):\n",
    "            previous_bytes = deque()\n",
    "            first_occurences = {}\n",
    "            byte_count = 0\n",
    "            with open(self.directory + \"/\" + file_name, \"rb\") as file:\n",
    "                #print(\"Reading %s\\n\" % self.directory + \"/\" + file_name)\n",
    "                my_string = \"\"\n",
    "                while True:\n",
    "                    byte_count += 1\n",
    "                    current_byte = file.read(1).hex()\n",
    "                    if not current_byte:\n",
    "                        break\n",
    "                    if byte_count >= ngrams:\n",
    "                        byte_gram = \"\".join(list(previous_bytes)) + current_byte\n",
    "                        if byte_gram not in first_occurences:\n",
    "                            first_occurences[byte_gram] = byte_count\n",
    "                        if byte_count % ngrams == 0:\n",
    "                            my_string += byte_gram + \" \"\n",
    "                        if ngrams > 1:\n",
    "                            previous_bytes.popleft()\n",
    "                    if ngrams > 1:\n",
    "                        previous_bytes.append(current_byte)\n",
    "                first_occurences_corpus[file_name] = first_occurences\n",
    "            doc_content.append(my_string)\n",
    "        doc_names = os.listdir(self.directory)\n",
    "\n",
    "        # Convert a collection of text documents to a matrix of token counts\n",
    "        vectorizer = TfidfVectorizer(use_idf=False)\n",
    "        # Learn the vocabulary dictionary and return term-document matrix.\n",
    "        x1 = vectorizer.fit_transform(doc_content).toarray()\n",
    "        del doc_content\n",
    "        self.vocab = [\"vocab\"]\n",
    "\n",
    "        self.vocab.extend(vectorizer.get_feature_names())\n",
    "        tdm = []\n",
    "        for i in range(len(doc_names)):\n",
    "            row = x1[i]\n",
    "            tdm.append(row)\n",
    "        svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)\n",
    "        reduced_tdm = svd.fit_transform(tdm)\n",
    "        tdm_first_occurences = []\n",
    "        self.corpus_names = doc_names\n",
    "        # Create a first occurences matrix that corresponds with the tdm\n",
    "        for j in range(len(doc_names)):\n",
    "            item = doc_names[j]\n",
    "            this_tdm = []\n",
    "            for i in range(0, len(tdm[0])):\n",
    "                word = self.vocab[i]\n",
    "                try:\n",
    "                    this_tdm.append(first_occurences_corpus[item][word])\n",
    "                except:\n",
    "                    this_tdm.append(0)\n",
    "            # print(this_tdm)\n",
    "            tdm_first_occurences.append(this_tdm)\n",
    "        reduced_tdm_first_occurences = svd.fit_transform(tdm_first_occurences)\n",
    "        del tdm_first_occurences\n",
    "        del tdm\n",
    "        tdt = [reduced_tdm, reduced_tdm_first_occurences]\n",
    "        self.tensor = tdt\n",
    "        #tdm_sparse = scipy.sparse.csr_matrix(tdm)\n",
    "        #tdm_first_occurences_sparse = scipy.sparse.csr_matrix(tdm_first_occurences)\n",
    "        if flag == 1:\n",
    "            print(\"  %s seconds for TDM Binary\" % format((time.time() - start_time1), '.2f'))\n",
    "        return self.tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class create_term_document_tensor_text_mixin:\n",
    "    def create_term_document_tensor_text(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates term-sentence-document tensor out of files in directory\n",
    "        Attempts to save this tensor to a pickle file\n",
    "        \n",
    "        :return: 3-D dense numpy array, self.tensor\n",
    "        \"\"\"\n",
    "        start_time2 = time.time()\n",
    "        if flag == 1:\n",
    "            print(\"Word-based Term Document Tensor\")\n",
    "\n",
    "        self.tensor = None\n",
    "        vectorizer = TfidfVectorizer(use_idf=False, analyzer=\"word\")\n",
    "        document_cutoff_positions = []\n",
    "        doc_content = []\n",
    "        pos = 0\n",
    "        max_matrix_height = 0\n",
    "        max_sentences = kwargs[\"lines\"]\n",
    "        self.corpus_names = os.listdir(self.directory)\n",
    "\n",
    "        # If given Pickle file, read it in\n",
    "        if self.file_name is not None:\n",
    "            file = open(self.file_name, 'rb')\n",
    "            self.tensor = pickle.load(file)\n",
    "            return self.tensor\n",
    "\n",
    "        # Create one large term document matrix from all documents. \n",
    "        # Done to ensure same vocabulary.\n",
    "        for file_name in self.corpus_names:\n",
    "            document_cutoff_positions.append(pos)\n",
    "            with open(self.directory + \"/\" + file_name, \"r\", errors=\"ignore\") as file:\n",
    "                print(\"Reading %s\" % self.directory + \"/\" + file_name)\n",
    "                for line in file:\n",
    "                    if len(line) > 2:\n",
    "                        pos += 1\n",
    "                        doc_content.append(line)\n",
    "                    if pos - document_cutoff_positions[-1] >= max_sentences:\n",
    "                        break\n",
    "                if max_matrix_height < pos - document_cutoff_positions[-1]:\n",
    "                    max_matrix_height = pos - document_cutoff_positions[-1]\n",
    "\n",
    "        document_cutoff_positions.append(pos)\n",
    "\n",
    "        x1 = vectorizer.fit_transform(doc_content)\n",
    "        matrix_length = len(vectorizer.get_feature_names())\n",
    "\n",
    "        # Split large term document matrix into term document tensor. \n",
    "        # Splits happen where one document ends.\n",
    "        for i in range(len(document_cutoff_positions) - 1):\n",
    "            temp = x1[document_cutoff_positions[i]:document_cutoff_positions[i + 1], :]\n",
    "            temp = temp.todense()\n",
    "            # Make all matrix slices the same size\n",
    "            term_sentence_matrix = np.zeros((max_matrix_height, matrix_length))\n",
    "            term_sentence_matrix[:temp.shape[0], :temp.shape[1]] = temp\n",
    "            if self.tensor is None:\n",
    "                self.tensor = term_sentence_matrix\n",
    "            else:\n",
    "                self.tensor = np.dstack((self.tensor, term_sentence_matrix))\n",
    "\n",
    "        self.file_name = self.directory + \".pkl\"\n",
    "        if flag == 1:\n",
    "            print(\"Finished tensor construction.\")\n",
    "        if flag == 1:\n",
    "            print(\"Tensor shape:\" + str(self.tensor.shape))\n",
    "        try:\n",
    "            pickle.dump(self.tensor, open(self.file_name, \"wb\"))\n",
    "        except OverflowError:\n",
    "            print(\"ERROR: Tensor cannot be saved to pickle file due to size larger than 4 GB\")\n",
    "        if flag == 1:\n",
    "            print(\"  %s seconds for TDM document\" % format((time.time() - start_time2),'.2f'))\n",
    "        return self.tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class parafac_decomposition_mixin:\n",
    "    def parafac_decomposition(self):\n",
    "        \"\"\"\n",
    "        Computes a parafac decomposition of the tensor.\n",
    "        This will return n rank 3 factor matrices, where n represents the \n",
    "        dimensionality of the tensor.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        start_time3 = time.time()\n",
    "        if flag == 1:\n",
    "            print(\"Ready to decompose the TDM\")\n",
    "        decompose = KruskalTensor(self.tensor.shape, rank=3, regularize=1e-6, \n",
    "                                  init='nvecs', X_data=self.tensor)\n",
    "        if flag == 1:\n",
    "            print(\"Returned from decomposing the TDM\")\n",
    "        self.factors = decompose.U\n",
    "        with tf.Session() as sess:\n",
    "            for i in range(len(self.factors)):\n",
    "                sess.run(self.factors[i].initializer)\n",
    "                self.factors[i] = self.factors[i].eval()\n",
    "        if flag == 1:\n",
    "            print(\"  %s seconds for decomposition of the tensor\" % \n",
    "                  format((time.time() - start_time3), '.2f'))\n",
    "        return self.factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TermDocumentTensor(generate_cosine_similarity_mixin, \n",
    "                         get_estimated_rank_mixin, \n",
    "                         create_term_document_tensor_mixin, \n",
    "                         create_binary_term_document_tensor_mixin,\n",
    "                         create_term_document_tensor_text_mixin, \n",
    "                         parafac_decomposition_mixin):\n",
    "    \n",
    "    def __init__(self, directory, type=\"binary\", file_name=None):\n",
    "        if flag==1:\n",
    "            print(\"Initializing the tensor\")\n",
    "        self.vocab = []\n",
    "        self.tensor = []\n",
    "        self.corpus_names = []\n",
    "        self.directory = directory\n",
    "        self.type = type\n",
    "        self.rank_approximation = None\n",
    "        self.factor_matrices = []\n",
    "        # These are the outputs of our tensor decomposition.\n",
    "        self.factors = []\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def print_formatted_term_document_tensor(self):\n",
    "        if flag == 1:\n",
    "            print(\"Print the Term Document Tensor\")\n",
    "        for matrix in self.tensor:\n",
    "            print(self.vocab)\n",
    "            for i in range(len(matrix)):\n",
    "                print(self.corpus_names[i], matrix[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files from output\n",
      "Initializing the tensor\n",
      "Creating a Term Document Tensor\n",
      "Word-based Term Document Tensor\n",
      "Reading output/JC.txt\n",
      "Reading output/1957-Eisenhower.txt\n",
      "Reading output/TNK.txt\n",
      "Reading output/Tmp.txt\n",
      "Reading output/1881-Garfield.txt\n",
      "Reading output/1905-Roosevelt.txt\n",
      "Reading output/1985-Reagan.txt\n",
      "Reading output/1913-Wilson.txt\n",
      "Reading output/Cym.txt\n",
      "Reading output/.DS_Store\n",
      "Reading output/2Henry4.txt\n",
      "Reading output/Cor.txt\n",
      "Reading output/1793-Washington.txt\n",
      "Reading output/1885-Cleveland.txt\n",
      "Reading output/Rom.txt\n",
      "Reading output/1789-Washington.txt\n",
      "Reading output/Mac.txt\n",
      "Reading output/2Henry6.txt\n",
      "Reading output/1845-Polk.txt\n",
      "Reading output/1925-Coolidge.txt\n",
      "Reading output/1837-VanBuren.txt\n",
      "Reading output/Henry5.txt\n",
      "Reading output/1941-Roosevelt.txt\n",
      "Reading output/Tim.txt\n",
      "Reading output/2001-Bush.txt\n",
      "Reading output/1961-Kennedy.txt\n",
      "Reading output/1909-Taft.txt\n",
      "Reading output/1945-Roosevelt.txt\n",
      "Reading output/1817-Monroe.txt\n",
      "Reading output/1949-Truman.txt\n",
      "Reading output/1953-Eisenhower.txt\n",
      "Reading output/Ven.txt\n",
      "Reading output/1841-Harrison.txt\n",
      "Reading output/John.txt\n",
      "Reading output/1889-Harrison.txt\n",
      "Reading output/1857-Buchanan.txt\n",
      "Reading output/2009-Obama.txt\n",
      "Reading output/1877-Hayes.txt\n",
      "Reading output/1861-Lincoln.txt\n",
      "Reading output/1973-Nixon.txt\n",
      "Reading output/MV.txt\n",
      "Reading output/1977-Carter.txt\n",
      "Reading output/3Henry6.txt\n",
      "Reading output/1993-Clinton.txt\n",
      "Reading output/1869-Grant.txt\n",
      "Reading output/1981-Reagan.txt\n",
      "Reading output/1937-Roosevelt.txt\n",
      "Reading output/1917-Wilson.txt\n",
      "Reading output/Err.txt\n",
      "Reading output/MM.txt\n",
      "Reading output/1849-Taylor.txt\n",
      "Reading output/WT.txt\n",
      "Reading output/TN.txt\n",
      "Reading output/Tro.txt\n",
      "Reading output/1933-Roosevelt.txt\n",
      "Reading output/Lear.txt\n",
      "Reading output/1809-Madison.txt\n",
      "Reading output/Ado.txt\n",
      "Reading output/2005-Bush.txt\n",
      "Reading output/TGVerona.txt\n",
      "Reading output/LLL.txt\n",
      "Reading output/Per.txt\n",
      "Reading output/1873-Grant.txt\n",
      "Reading output/Tit.txt\n",
      "Reading output/1921-Harding.txt\n",
      "Reading output/AntCleo.txt\n",
      "Reading output/1829-Jackson.txt\n",
      "Reading output/1969-Nixon.txt\n",
      "Reading output/1805-Jefferson.txt\n",
      "Reading output/1801-Jefferson.txt\n",
      "Reading output/Ham.txt\n",
      "Reading output/Henry8.txt\n",
      "Reading output/1901-McKinley.txt\n",
      "Reading output/1853-Pierce.txt\n",
      "Reading output/Shr.txt\n",
      "Reading output/AllsWell.txt\n",
      "Reading output/1813-Madison.txt\n",
      "Reading output/1929-Hoover.txt\n",
      "Reading output/1893-Cleveland.txt\n",
      "Reading output/AsYouLike.txt\n",
      "Reading output/1989-Bush.txt\n",
      "Reading output/1897-McKinley.txt\n",
      "Reading output/1797-Adams.txt\n",
      "Reading output/1833-Jackson.txt\n",
      "Reading output/1965-Johnson.txt\n",
      "Reading output/1Henry4.txt\n",
      "Reading output/Richard2.txt\n",
      "Reading output/1997-Clinton.txt\n",
      "Reading output/1Henry6.txt\n",
      "Reading output/Oth.txt\n",
      "Reading output/MND.txt\n",
      "Reading output/1821-Monroe.txt\n",
      "Reading output/1825-Adams.txt\n",
      "Reading output/Wiv.txt\n",
      "Reading output/1865-Lincoln.txt\n",
      "Reading output/Richard3.txt\n",
      "Finished tensor construction.\n",
      "Tensor shape:(300, 14051, 96)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-83a2e74bd72e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     print(\"  %s seconds is the total time for program to execute\" % \n\u001b[1;32m     38\u001b[0m           format((time.time() - start_time), '.2f'))\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-83a2e74bd72e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reading files from %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'directory'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtdt\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mTermDocumentTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'directory'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_term_document_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ngrams'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lines'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decom'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"parafac\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mfactors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparafac_decomposition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-20ed6bcf6b64>\u001b[0m in \u001b[0;36mcreate_term_document_tensor\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_binary_term_document_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_term_document_tensor_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-d0f121e78f35>\u001b[0m in \u001b[0;36mcreate_term_document_tensor_text\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor shape:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOverflowError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ERROR: Tensor cannot be saved to pickle file due to size larger than 4 GB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "# To run this notebook , you need to have the directory = \"output\" in the same folder \n",
    "# where this folder is executed \n",
    "\n",
    "args = {}\n",
    "args['Comments'] = \"N\"\n",
    "args['axis'] = 2\n",
    "args['binary'] = False\n",
    "args['components'] = 2\n",
    "args['decom'] = 'parafac'\n",
    "args['directory'] = 'output'   # was 'output' which I don't understand\n",
    "args['file'] = None\n",
    "args['heatmap'] = True\n",
    "args['kmeans']= True\n",
    "args['lines'] = 300                              # used to be only 100, which seems short\n",
    "args['ngrams'] = 1\n",
    "args['text'] = True\n",
    "args['output_option'] = False\n",
    "\n",
    "# create the tensor and its decompositions\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    file_type = \"binary\" if args['binary'] else \"text\"\n",
    "    print(\"Reading files from %s\" % args['directory'])\n",
    "    tdt =TermDocumentTensor(args['directory'], type=file_type, file_name=args['file'])\n",
    "    tdt.create_term_document_tensor(ngrams=args['ngrams'], lines=args['lines'])\n",
    "    if args['decom'] == \"parafac\":\n",
    "        factors = tdt.parafac_decomposition()\n",
    "    visualize = TensorVisualization()\n",
    "    if args['heatmap']:\n",
    "        cos_sim = tdt.generate_cosine_similarity_matrix(factors[args['axis']])\n",
    "        visualize.generate_heat_map(cos_sim, tdt.corpus_names)\n",
    "    if args['kmeans']:\n",
    "        visualize.k_means_clustering(factors[args['axis']], tdt.corpus_names, \n",
    "                                     clusters=args['components'])\n",
    "        # what is this doing here?  if we're not using it\n",
    "        #tdt.generate_cosine_similarity_matrix(factors[args['axis']])\n",
    "    print(\"  %s seconds is the total time for program to execute\" % \n",
    "          format((time.time() - start_time), '.2f'))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
