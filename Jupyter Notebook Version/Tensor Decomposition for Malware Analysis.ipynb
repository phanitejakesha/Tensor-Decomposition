{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Decomposition for Malware Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Report for August 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning of the Spring 2018 semester, we wrote the first version of our tensor decomposition code, using the works of Shakespeare, and a collection of Inaugural Addresses from Presidents of the USA, as test corpora.  Our goal was simple: to see if we could build tensors from these data sources, and do some basic analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This effort was successful, and the results were presented as posters and shared with the sponsors.  In brief, we were able to see that the data formed clusters in ways that separated the Shakespeare from the Inaugural addresses.  However, we were not able to see any structure *within* the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The undergraduate students working on the project up to this time, namely the end of the Spring 2018 semester, graduated and left the project.  By that time Nicholas had become more comfortable with Jupyter Notebooks, and he took a somewhat more active role in software development in the early summer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first attempt at structuring the data in a tensor format used the file names, the terms in the corpus, and the line numbers at which terms might appear, as dimensions.  That is, an entry $M_{i,j,k}$ in the tensor would be an integer representing the number of times term $i$ occurred in line $j$ of file $k$.  To manage the size of the tensor in memory, we restricted the line numbers to the first thousand lines.  The Shakespeare corpus has only 37 files, but there are still many thousands of terms, and we skipped many lines of data.  **Hamlet**, for example, has over 4000 lines of text.  The resulting tensor turned out to lose data, and was quite spare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Notebook version of the code, which follows this report in this notebook, has been broken up into a modular format, with each of the Python classes being in its own cell.  This will make the code easier to change, and has already made it easier to document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also planning to try different ways of structuring the data into  a tensor format.  We will keep the file name as one of the dimensions.  We will keep the term set as another dimension, but will limit the size of the term set by using a term weighting scheme such as the well-known tf.idf used in Information Retrieval, and discarding terms that are below are certain threshold in weighted frequency.  We want to preserve location information, but line numbers aren't the right unit.  For PE files, i.e. executable binaries, the section type (\"text\", \"resource\", \"header\", \"import\", \"upx\", among others) will be tried.  For text, a quartile (or quintile) approach might make sense: Shakespeare consistently sructured his plays into five acts of roughly equal length, so terms in Act II would be counted separately from terms in Act V."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the early phase of this work, we used heatmaps and k-means clustering, provided by the Python libraries, to visualize the results.  While this may be useful at times, it makes too little use of the tensor decomposition.  The PARAFAC decomposition (as the reader will recall) expresses the tensor as a sum of rank one third-order tensors.  (See Figure 3.1 in Kolda's SIAM 2009 paper.)  We claim that each of these third order tensors can be visualized, one by one, using 3-D scatterplot or similar functions available in Python.  We intend to try this soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we note that Dr. Nicholas has been invited to present this work at a SIAM workshop to be held in Oregon in February 2019.  (Still waiting for details.)  The project has been using funds at a somewhat slower rate than may have been necessary, but the delays involved in moving money from NSF to UMBC have made us cautious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of progress report.  \n",
    "\n",
    "Notebook code and documentation follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have code like this at the top of every notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "\n",
    "#from LZJD import *         # don't seem to need LZJD in this notebook, yet\n",
    "#from whereAmI import *\n",
    "from os import chdir\n",
    "from os import getcwd\n",
    "from os import listdir\n",
    "from os import path\n",
    "\n",
    "#(home, dropBoxDir, dataDir) = whereAmI()\n",
    "\n",
    "#print(dropBoxDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing TensorFlow\n",
    "\n",
    "We need TensorFlow to do the machine learning work in this notebook.  To install tensorflow on a Ubuntu guest, try this:\n",
    "\n",
    "pip install -U tensorflow\n",
    "\n",
    "(contrary to the documentation at \n",
    "https://www.tensorflow.org/install/install_linux#InstallingNativePip,\n",
    "it doesn't seem that sudo is necessary.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if tensorflow is installed, try the following as a stand-alone script.  The cell shown below is markup, and is not intended for execution inside this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`python -c \"import tensorflow as tf; print(tf.__version__)\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that works, then the following cell should also work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing tensorflow the first time, there was this warning:\n",
    "    \n",
    "`/home/charles/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: \n",
    "FutureWarning: Conversion of the second argument of issubdtype from \n",
    "\"float\" to \"np.floating\" is deprecated. In future, it will be treated \n",
    "as \"np.float64 == np.dtype(float).type\".  from ._conv import register_converters as _register_converters`\n",
    "\n",
    "upgrading to a newer version of h5py seems to make the warning go away.\n",
    "\n",
    "`pip install h5py==2.8.0rc1`\n",
    "\n",
    "`python -c 'import h5py; print(h5py.version.info)'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some options for the program..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to have some tensor decomposition code that does not come with tensorflow, but uses it.  See https://github.com/ebigelow/tf-decompose.  The pip program doesn't seem to know about ktensor and its friends, so unzip the code from GitHub and copy ktensor.py, dtensor.py, and utils.py to the Jupyter process's directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor decomposition code uses tqdm, which looks handy anyway, and is easily installed:\n",
    "    `pip install tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ktensor import KruskalTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can parse the Shakespeare corpus, which I have in ~/Dropbox/working/vx/WS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining the class termdocumenttensor which has the functions like creating the tensor,\n",
    "# decomposing the tensor, generation of the similiarity matrix and its clustering \n",
    "\n",
    "from scipy import spatial\n",
    "from collections import deque\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import _pickle as pickle\n",
    "import time\n",
    "\n",
    "flag = 1   # print comments in a few places\n",
    "\n",
    "def flag_function_tdm(cmts):\n",
    "        global flag\n",
    "        flag = cmts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code needs plotly.  That's easy to install: `pip install plotly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tensorvisualization class which takes the similarity matrix and vizualizes \n",
    "# them into Clusters or Heatmaps\n",
    "flag =1\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import time\n",
    "import scipy.cluster.hierarchy\n",
    "import scipy.spatial.distance\n",
    "from collections import Counter\n",
    "#import accuracy    # Phani, is this package important?\n",
    "\n",
    "def flag_function_visualization(cmts):\n",
    "    global flag\n",
    "    flag = cmts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be several of these classes that end in \"mixin\".  By having no data in those classes, only methods, often just one method, they can be inherited by another class.  That allows us to have methods in their own cells in this notebook, allowing markup cells between them as documentation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class generate_heat_map_mixin:\n",
    "    def generate_heat_map(self, data, axis_labels):\n",
    "        \"\"\"\n",
    "        Generates a heat map for the current data\n",
    "        Currently only meant to support using a cosine similarity matrix\n",
    "        :param data:\n",
    "        :param axis_labels:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if flag == 1:\n",
    "            print(\"Generating heatmap...\")\n",
    "        axis_labels_abbreviated = [label[:14] for label in axis_labels]\n",
    "        info = [go.Heatmap(z=data,\n",
    "                           x=axis_labels_abbreviated,\n",
    "                           y=axis_labels_abbreviated,\n",
    "                           colorscale='Hot',\n",
    "                           )]\n",
    "\n",
    "        layout = go.Layout(title='Cosine Similarity Between Documents',\n",
    "                           xaxis=dict(ticks=''),\n",
    "                           yaxis=dict(ticks=''),\n",
    "                           plot_bgcolor='#444',\n",
    "                           paper_bgcolor='#eee'\n",
    "                           )\n",
    "        fig = go.Figure(data=info, layout=layout)\n",
    "        plotly.offline.plot(fig, filename='notebook_heatmap.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use SVD to do something, and then some clustering.  Not sure exactly what the underlying logic might be..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class k_mean_clustering_mixin:\n",
    "    def k_means_clustering(self, factor_matrix, file_names=[], clusters=2):\n",
    "        clusters = 2\n",
    "        svd = TruncatedSVD(n_components=2, n_iter=20, random_state=42)\n",
    "        reduced = svd.fit_transform(factor_matrix)\n",
    "        kmeans = KMeans(n_clusters=clusters, random_state=0).fit(factor_matrix)\n",
    "        labels_predicted = kmeans.labels_\n",
    "        data = [plotly.graph_objs.Scatter(x=[entry[0] for entry in reduced],\n",
    "                                          y=[entry[1] for entry in reduced],\n",
    "                                          mode='markers',\n",
    "                                          marker=dict(color=kmeans.labels_),\n",
    "                                          text=file_names\n",
    "                                          )\n",
    "                ]\n",
    "        fig = go.Figure(data=data)\n",
    "        plotly.offline.plot(fig, filename='kmeans_cluster.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TensorVisualization(generate_heat_map_mixin, k_mean_clustering_mixin):\n",
    "    def __init__(self):\n",
    "        from plotly import __version__\n",
    "        print( \"Using Plotly version \"+__version__) # requires version >= 1.9.0\n",
    "        plotly.tools.set_credentials_file(username='cknicholas', \n",
    "                                  api_key='pa9Z110GEeh029O4jTV0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class generate_cosine_similarity_mixin:\n",
    "    def generate_cosine_similarity_matrix(self, matrix):\n",
    "        f = open('cosine.txt', 'w');\n",
    "        if flag == 1:\n",
    "            print(\"Generating a cosine similarity matrix\")\n",
    "        cosine_sim = []\n",
    "        for entry in matrix:\n",
    "            sim = []\n",
    "            for other_entry in matrix:\n",
    "                sim.append(spatial.distance.cosine(entry, other_entry) * -1 + 1)\n",
    "                f.write(str(spatial.distance.cosine(entry, other_entry) * -1 + 1))\n",
    "                f.write(\"\\n\")\n",
    "            cosine_sim.append(sim)\n",
    "        return cosine_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This logic for estimating the rank of the tensor is interesting.  But I'm not sure if it's being used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class get_estimated_rank_mixin:\n",
    "    def get_estimated_rank(self):\n",
    "        \"\"\"\n",
    "        Getting the rank of a tensor is an NP hard problem\n",
    "        Therefore we use an estimation based on the size of the dimensions of our tensor.\n",
    "        These numbers are taken from Table 3.3 of Tammy Kolda's paper:\n",
    "        http://www.sandia.gov/~tgkolda/pubs/pubfiles/TensorReview.pdf\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # At the moment the rank returned by this function is normally too high for either\n",
    "        # my machine or the tensorly library to handle, therefore I have made it just \n",
    "        # return 1 for right now\n",
    "        if flag == 1:\n",
    "            print(\"Estimating the rank of the tensor...\")\n",
    "        I = len(self.tensor[0])\n",
    "        J = len(self.tensor[0][0])\n",
    "        K = len(self.tensor)\n",
    "\n",
    "        if I == 1 or J == 1 or K == 1:\n",
    "            return 1\n",
    "        elif I == J == K == 2:\n",
    "            return 2\n",
    "        elif I == J == 3 and K == 2:\n",
    "            return 3\n",
    "        elif I == 5 and J == K == 3:\n",
    "            return 5\n",
    "        elif I >= 2 * J and K == 2:\n",
    "            return 2 * J\n",
    "        elif 2 * J > I > J and K == 2:\n",
    "            return I\n",
    "        elif I == J and K == 2:\n",
    "            return I\n",
    "        elif I >= J * K:\n",
    "            return J * K\n",
    "        elif J * K - J < I < J * K:\n",
    "            return I\n",
    "        elif I == J * K - I:\n",
    "            return I\n",
    "        else:\n",
    "            print(I, J, K, \"did not have an exact estimation\")\n",
    "            return min(I * J, I * K, J * K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class create_term_document_tensor_mixin:\n",
    "    def create_term_document_tensor(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Generic tensor creation function. Returns different tensor based on user input.\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if flag == 1:\n",
    "            print(\"Creating a Term Document Tensor\")\n",
    "        if self.type == \"binary\":\n",
    "            return self.create_binary_term_document_tensor(**kwargs)\n",
    "        else:\n",
    "            return self.create_term_document_tensor_text(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class create_binary_term_document_tensor_mixin:\n",
    "    def create_binary_term_document_tensor(self, **kwargs):\n",
    "        start_time1 = time.time()\n",
    "        if flag == 1:\n",
    "            print(\"Binary Term Document Tensor\")\n",
    "        doc_content = []\n",
    "        first_occurences_corpus = {}\n",
    "        ngrams = kwargs[\"ngrams\"] if kwargs[\"ngrams\"] is not None else 1\n",
    "        print(\"ngrams is %s\" % (ngrams))\n",
    "\n",
    "        for file_name in os.listdir(self.directory):\n",
    "            previous_bytes = deque()\n",
    "            first_occurences = {}\n",
    "            byte_count = 0\n",
    "            with open(self.directory + \"/\" + file_name, \"rb\") as file:\n",
    "                #print(\"Reading %s\\n\" % self.directory + \"/\" + file_name)\n",
    "                my_string = \"\"\n",
    "                while True:\n",
    "                    byte_count += 1\n",
    "                    current_byte = file.read(1).hex()\n",
    "                    if not current_byte:\n",
    "                        break\n",
    "                    if byte_count >= ngrams:\n",
    "                        byte_gram = \"\".join(list(previous_bytes)) + current_byte\n",
    "                        if byte_gram not in first_occurences:\n",
    "                            first_occurences[byte_gram] = byte_count\n",
    "                        if byte_count % ngrams == 0:\n",
    "                            my_string += byte_gram + \" \"\n",
    "                        if ngrams > 1:\n",
    "                            previous_bytes.popleft()\n",
    "                    if ngrams > 1:\n",
    "                        previous_bytes.append(current_byte)\n",
    "                first_occurences_corpus[file_name] = first_occurences\n",
    "            doc_content.append(my_string)\n",
    "        doc_names = os.listdir(self.directory)\n",
    "\n",
    "        # Convert a collection of text documents to a matrix of token counts\n",
    "        vectorizer = TfidfVectorizer(use_idf=False)\n",
    "        # Learn the vocabulary dictionary and return term-document matrix.\n",
    "        x1 = vectorizer.fit_transform(doc_content).toarray()\n",
    "        del doc_content\n",
    "        self.vocab = [\"vocab\"]\n",
    "\n",
    "        self.vocab.extend(vectorizer.get_feature_names())\n",
    "        tdm = []\n",
    "        for i in range(len(doc_names)):\n",
    "            row = x1[i]\n",
    "            tdm.append(row)\n",
    "        svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)\n",
    "        reduced_tdm = svd.fit_transform(tdm)\n",
    "        tdm_first_occurences = []\n",
    "        self.corpus_names = doc_names\n",
    "        # Create a first occurences matrix that corresponds with the tdm\n",
    "        for j in range(len(doc_names)):\n",
    "            item = doc_names[j]\n",
    "            this_tdm = []\n",
    "            for i in range(0, len(tdm[0])):\n",
    "                word = self.vocab[i]\n",
    "                try:\n",
    "                    this_tdm.append(first_occurences_corpus[item][word])\n",
    "                except:\n",
    "                    this_tdm.append(0)\n",
    "            # print(this_tdm)\n",
    "            tdm_first_occurences.append(this_tdm)\n",
    "        reduced_tdm_first_occurences = svd.fit_transform(tdm_first_occurences)\n",
    "        del tdm_first_occurences\n",
    "        del tdm\n",
    "        tdt = [reduced_tdm, reduced_tdm_first_occurences]\n",
    "        self.tensor = tdt\n",
    "        #tdm_sparse = scipy.sparse.csr_matrix(tdm)\n",
    "        #tdm_first_occurences_sparse = scipy.sparse.csr_matrix(tdm_first_occurences)\n",
    "        if flag == 1:\n",
    "            print(\"  %s seconds for TDM Binary\" % format((time.time() - start_time1), '.2f'))\n",
    "        return self.tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class create_term_document_tensor_text_mixin:\n",
    "    def create_term_document_tensor_text(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates term-sentence-document tensor out of files in directory\n",
    "        Attempts to save this tensor to a pickle file\n",
    "        \n",
    "        :return: 3-D dense numpy array, self.tensor\n",
    "        \"\"\"\n",
    "        start_time2 = time.time()\n",
    "        if flag == 1:\n",
    "            print(\"Word-based Term Document Tensor\")\n",
    "\n",
    "        self.tensor = None\n",
    "        vectorizer = TfidfVectorizer(use_idf=False, analyzer=\"word\")\n",
    "        document_cutoff_positions = []\n",
    "        doc_content = []\n",
    "        pos = 0\n",
    "        max_matrix_height = 0\n",
    "        max_sentences = kwargs[\"lines\"]\n",
    "        self.corpus_names = os.listdir(self.directory)\n",
    "\n",
    "        # If given Pickle file, read it in\n",
    "        if self.file_name is not None:\n",
    "            file = open(self.file_name, 'rb')\n",
    "            self.tensor = pickle.load(file)\n",
    "            return self.tensor\n",
    "\n",
    "        # Create one large term document matrix from all documents. \n",
    "        # Done to ensure same vocabulary.\n",
    "        for file_name in self.corpus_names:\n",
    "            document_cutoff_positions.append(pos)\n",
    "            with open(self.directory + \"/\" + file_name, \"r\", errors=\"ignore\") as file:\n",
    "                print(\"Reading %s\" % self.directory + \"/\" + file_name)\n",
    "                for line in file:\n",
    "                    if len(line) > 2:\n",
    "                        pos += 1\n",
    "                        doc_content.append(line)\n",
    "                    if pos - document_cutoff_positions[-1] >= max_sentences:\n",
    "                        break\n",
    "                if max_matrix_height < pos - document_cutoff_positions[-1]:\n",
    "                    max_matrix_height = pos - document_cutoff_positions[-1]\n",
    "\n",
    "        document_cutoff_positions.append(pos)\n",
    "\n",
    "        x1 = vectorizer.fit_transform(doc_content)\n",
    "        matrix_length = len(vectorizer.get_feature_names())\n",
    "\n",
    "        # Split large term document matrix into term document tensor. \n",
    "        # Splits happen where one document ends.\n",
    "        for i in range(len(document_cutoff_positions) - 1):\n",
    "            temp = x1[document_cutoff_positions[i]:document_cutoff_positions[i + 1], :]\n",
    "            temp = temp.todense()\n",
    "            # Make all matrix slices the same size\n",
    "            term_sentence_matrix = np.zeros((max_matrix_height, matrix_length))\n",
    "            term_sentence_matrix[:temp.shape[0], :temp.shape[1]] = temp\n",
    "            if self.tensor is None:\n",
    "                self.tensor = term_sentence_matrix\n",
    "            else:\n",
    "                self.tensor = np.dstack((self.tensor, term_sentence_matrix))\n",
    "\n",
    "        self.file_name = self.directory + \".pkl\"\n",
    "        if flag == 1:\n",
    "            print(\"Finished tensor construction.\")\n",
    "        if flag == 1:\n",
    "            print(type(self.tensor))\n",
    "        try:\n",
    "            pickle.dump(self.tensor, open(self.file_name, \"wb\"))\n",
    "        except OverflowError:\n",
    "            print(\"ERROR: Tensor cannot be saved to pickle file due to size larger than 4 GB\")\n",
    "        if flag == 1:\n",
    "            print(\"  %s seconds for TDM document\" % format((time.time() - start_time2),'.2f'))\n",
    "        return self.tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class parafac_decomposition_mixin:\n",
    "    def parafac_decomposition(self):\n",
    "        \"\"\"\n",
    "        Computes a parafac decomposition of the tensor.\n",
    "        This will return n rank 3 factor matrices, where n represents the \n",
    "        dimensionality of the tensor.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        start_time3 = time.time()\n",
    "        if flag == 1:\n",
    "            print(\"Ready to decompose the TDM\")\n",
    "        decompose = KruskalTensor(self.tensor.shape, rank=3, regularize=1e-6, \n",
    "                                  init='nvecs', X_data=self.tensor)\n",
    "        if flag == 1:\n",
    "            print(\"Returned from decomposing the TDM\")\n",
    "        self.factors = decompose.U\n",
    "        with tf.Session() as sess:\n",
    "            for i in range(len(self.factors)):\n",
    "                sess.run(self.factors[i].initializer)\n",
    "                self.factors[i] = self.factors[i].eval()\n",
    "        if flag == 1:\n",
    "            print(\"  %s seconds for decomposition of the tensor\" % \n",
    "                  format((time.time() - start_time3), '.2f'))\n",
    "        \n",
    "        print(self.factors[0])\n",
    "        print(self.factors[1])\n",
    "        print(self.factors[2])\n",
    "        return self.factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TermDocumentTensor(generate_cosine_similarity_mixin, \n",
    "                         get_estimated_rank_mixin, \n",
    "                         create_term_document_tensor_mixin, \n",
    "                         create_binary_term_document_tensor_mixin,\n",
    "                         create_term_document_tensor_text_mixin, \n",
    "                         parafac_decomposition_mixin):\n",
    "    \n",
    "    def __init__(self, directory, type=\"binary\", file_name=None):\n",
    "        if flag==1:\n",
    "            print(\"Initializing the tensor\")\n",
    "        self.vocab = []\n",
    "        self.tensor = []\n",
    "        self.corpus_names = []\n",
    "        self.directory = directory\n",
    "        self.type = type\n",
    "        self.rank_approximation = None\n",
    "        self.factor_matrices = []\n",
    "        # These are the outputs of our tensor decomposition.\n",
    "        self.factors = []\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def print_formatted_term_document_tensor(self):\n",
    "        if flag == 1:\n",
    "            print(\"Print the Term Document Tensor\")\n",
    "        for matrix in self.tensor:\n",
    "            print(self.vocab)\n",
    "            for i in range(len(matrix)):\n",
    "                print(self.corpus_names[i], matrix[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files from shakespeare_president\n",
      "Initializing the tensor\n",
      "Creating a Term Document Tensor\n",
      "Word-based Term Document Tensor\n",
      "Reading shakespeare_president/JC.txt\n",
      "Reading shakespeare_president/1957-Eisenhower.txt\n",
      "Reading shakespeare_president/TNK.txt\n",
      "Reading shakespeare_president/Tmp.txt\n",
      "Reading shakespeare_president/1881-Garfield.txt\n",
      "Reading shakespeare_president/1905-Roosevelt.txt\n",
      "Reading shakespeare_president/1985-Reagan.txt\n",
      "Reading shakespeare_president/1913-Wilson.txt\n",
      "Reading shakespeare_president/Cym.txt\n",
      "Reading shakespeare_president/2Henry4.txt\n",
      "Reading shakespeare_president/Cor.txt\n",
      "Reading shakespeare_president/1793-Washington.txt\n",
      "Reading shakespeare_president/1885-Cleveland.txt\n",
      "Reading shakespeare_president/Rom.txt\n",
      "Reading shakespeare_president/1789-Washington.txt\n",
      "Reading shakespeare_president/Mac.txt\n",
      "Reading shakespeare_president/2Henry6.txt\n",
      "Reading shakespeare_president/1845-Polk.txt\n",
      "Reading shakespeare_president/1925-Coolidge.txt\n",
      "Reading shakespeare_president/1837-VanBuren.txt\n",
      "Reading shakespeare_president/Henry5.txt\n",
      "Reading shakespeare_president/1941-Roosevelt.txt\n",
      "Reading shakespeare_president/Tim.txt\n",
      "Reading shakespeare_president/.pkl\n",
      "Reading shakespeare_president/2001-Bush.txt\n",
      "Reading shakespeare_president/1961-Kennedy.txt\n",
      "Reading shakespeare_president/1909-Taft.txt\n",
      "Reading shakespeare_president/1945-Roosevelt.txt\n",
      "Reading shakespeare_president/1817-Monroe.txt\n",
      "Reading shakespeare_president/1949-Truman.txt\n",
      "Reading shakespeare_president/1953-Eisenhower.txt\n",
      "Reading shakespeare_president/Ven.txt\n",
      "Reading shakespeare_president/1841-Harrison.txt\n",
      "Reading shakespeare_president/John.txt\n",
      "Reading shakespeare_president/1889-Harrison.txt\n",
      "Reading shakespeare_president/1857-Buchanan.txt\n",
      "Reading shakespeare_president/2009-Obama.txt\n",
      "Reading shakespeare_president/1877-Hayes.txt\n",
      "Reading shakespeare_president/1861-Lincoln.txt\n",
      "Reading shakespeare_president/1973-Nixon.txt\n",
      "Reading shakespeare_president/MV.txt\n",
      "Reading shakespeare_president/1977-Carter.txt\n",
      "Reading shakespeare_president/3Henry6.txt\n",
      "Reading shakespeare_president/1993-Clinton.txt\n",
      "Reading shakespeare_president/1869-Grant.txt\n",
      "Reading shakespeare_president/1981-Reagan.txt\n",
      "Reading shakespeare_president/1937-Roosevelt.txt\n",
      "Reading shakespeare_president/1917-Wilson.txt\n",
      "Reading shakespeare_president/Err.txt\n",
      "Reading shakespeare_president/MM.txt\n",
      "Reading shakespeare_president/1849-Taylor.txt\n",
      "Reading shakespeare_president/WT.txt\n",
      "Reading shakespeare_president/TN.txt\n",
      "Reading shakespeare_president/Tro.txt\n",
      "Reading shakespeare_president/1933-Roosevelt.txt\n",
      "Reading shakespeare_president/Lear.txt\n",
      "Reading shakespeare_president/1809-Madison.txt\n",
      "Reading shakespeare_president/Ado.txt\n",
      "Reading shakespeare_president/2005-Bush.txt\n",
      "Reading shakespeare_president/TGVerona.txt\n",
      "Reading shakespeare_president/LLL.txt\n",
      "Reading shakespeare_president/Per.txt\n",
      "Reading shakespeare_president/1873-Grant.txt\n",
      "Reading shakespeare_president/Tit.txt\n",
      "Reading shakespeare_president/1921-Harding.txt\n",
      "Reading shakespeare_president/AntCleo.txt\n",
      "Reading shakespeare_president/1829-Jackson.txt\n",
      "Reading shakespeare_president/1969-Nixon.txt\n",
      "Reading shakespeare_president/1805-Jefferson.txt\n",
      "Reading shakespeare_president/1801-Jefferson.txt\n",
      "Reading shakespeare_president/Ham.txt\n",
      "Reading shakespeare_president/Henry8.txt\n",
      "Reading shakespeare_president/1901-McKinley.txt\n",
      "Reading shakespeare_president/1853-Pierce.txt\n",
      "Reading shakespeare_president/Shr.txt\n",
      "Reading shakespeare_president/AllsWell.txt\n",
      "Reading shakespeare_president/1813-Madison.txt\n",
      "Reading shakespeare_president/1929-Hoover.txt\n",
      "Reading shakespeare_president/1893-Cleveland.txt\n",
      "Reading shakespeare_president/AsYouLike.txt\n",
      "Reading shakespeare_president/1989-Bush.txt\n",
      "Reading shakespeare_president/1897-McKinley.txt\n",
      "Reading shakespeare_president/1797-Adams.txt\n",
      "Reading shakespeare_president/1833-Jackson.txt\n",
      "Reading shakespeare_president/1965-Johnson.txt\n",
      "Reading shakespeare_president/1Henry4.txt\n",
      "Reading shakespeare_president/Richard2.txt\n",
      "Reading shakespeare_president/1997-Clinton.txt\n",
      "Reading shakespeare_president/1Henry6.txt\n",
      "Reading shakespeare_president/Oth.txt\n",
      "Reading shakespeare_president/MND.txt\n",
      "Reading shakespeare_president/1821-Monroe.txt\n",
      "Reading shakespeare_president/1825-Adams.txt\n",
      "Reading shakespeare_president/Wiv.txt\n",
      "Reading shakespeare_president/1865-Lincoln.txt\n",
      "Reading shakespeare_president/Richard3.txt\n",
      "Finished tensor construction.\n",
      "<class 'numpy.ndarray'>\n",
      "  71.92 seconds for TDM document\n",
      "Ready to decompose the TDM\n",
      "Returned from decomposing the TDM\n",
      "  236.07 seconds for decomposition of the tensor\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'factors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-151f479cf295>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     print(\"  %s seconds is the total time for program to execute\" % \n\u001b[1;32m     38\u001b[0m           format((time.time() - start_time), '.2f'))\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-151f479cf295>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_term_document_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ngrams'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lines'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decom'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"parafac\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mfactors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparafac_decomposition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mvisualize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorVisualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'heatmap'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-08efce4ccaf1>\u001b[0m in \u001b[0;36mparafac_decomposition\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m                   format((time.time() - start_time3), '.2f'))\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'factors' is not defined"
     ]
    }
   ],
   "source": [
    "# To run this notebook , you need to have the directory = \"output\" in the same folder \n",
    "# where this folder is executed \n",
    "\n",
    "args = {}\n",
    "args['Comments'] = \"N\"\n",
    "args['axis'] = 2\n",
    "args['binary'] = False\n",
    "args['components'] = 2\n",
    "args['decom'] = 'parafac'\n",
    "args['directory'] = 'shakespeare_president'    # was 'output' which I don't understand\n",
    "args['file'] = None\n",
    "args['heatmap'] = True\n",
    "args['kmeans']= True\n",
    "args['lines'] = 100                              # used to be only 100, which seems short\n",
    "args['ngrams'] = 1\n",
    "args['text'] = True\n",
    "args['output_option'] = False\n",
    "\n",
    "# create the tensor and its decompositions\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    file_type = \"binary\" if args['binary'] else \"text\"\n",
    "    print(\"Reading files from %s\" % args['directory'])\n",
    "    tdt =TermDocumentTensor(args['directory'], type=file_type, file_name=args['file'])\n",
    "    tdt.create_term_document_tensor(ngrams=args['ngrams'], lines=args['lines'])\n",
    "    if args['decom'] == \"parafac\":\n",
    "        factors = tdt.parafac_decomposition()\n",
    "    visualize = TensorVisualization()\n",
    "    if args['heatmap']:\n",
    "        cos_sim = tdt.generate_cosine_similarity_matrix(factors[args['axis']])\n",
    "        visualize.generate_heat_map(cos_sim, tdt.corpus_names)\n",
    "    if args['kmeans']:\n",
    "        visualize.k_means_clustering(factors[args['axis']], tdt.corpus_names, \n",
    "                                     clusters=args['components'])\n",
    "        # what is this doing here?  if we're not using it\n",
    "        #tdt.generate_cosine_similarity_matrix(factors[args['axis']])\n",
    "    print(\"  %s seconds is the total time for program to execute\" % \n",
    "          format((time.time() - start_time), '.2f'))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
