{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Python bash script to find all the strings in a executable \n",
    "import os\n",
    "os.system('for i in `ls TestFiles/ | xargs` ; do `strings TestFiles/$i > textFiles/$i.txt`; done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now these text files needs to be analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All the imports for the program go here\n",
    "import time\n",
    "import sys\n",
    "from os import chdir\n",
    "from os import getcwd\n",
    "from os import path\n",
    "import os\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from ktensor import KruskalTensor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Phaniteja/Desktop/Github/Tensor-Decomposition\n",
      "Found 191 files to look at\n"
     ]
    }
   ],
   "source": [
    "# This block takes the present working directory \n",
    "oldwd = getcwd()\n",
    "#dataDir = oldwd+\"/LZsetUpdated/zeusBinary/\"\n",
    "#dataDir = oldwd+\"/TermDocumentTensor/test_files/\"\n",
    "dataDir = \"/Users/Phaniteja/desktop/Github/Tensor-Decomposition/textFiles/\"\n",
    "print(oldwd)\n",
    "text_files = [f for f in os.listdir(dataDir)]# if f.endswith('.exe')]\n",
    "numberOfFiles = len(text_files)\n",
    "print(\"Found %d files to look at\" % numberOfFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "malwareCount=[]\n",
    "benignCount= []\n",
    "for i in range(0,len(text_files)):\n",
    "    if text_files[i][0:4]==\"zeus\":\n",
    "        malwareCount.append(text_files[i])\n",
    "    else:\n",
    "        benignCount.append(text_files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121 70\n"
     ]
    }
   ],
   "source": [
    "print(len(benignCount),len(malwareCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All the initilizations in the project \n",
    "unpacked_files = []\n",
    "count = 0\n",
    "mostFrequentNgrams = []\n",
    "length_Ngram_Dictionaries = []\n",
    "global_Dictionary_ngrams = {}\n",
    "file_Dictionary_ngram = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#definition for the accuracy of the given dataset\n",
    "def findAccuracies(groundTruth,predictedLabel):\n",
    "    real_Label = []\n",
    "    for i in range(0,len(groundTruth)):\n",
    "        if groundTruth[i][:4]==\"zeus\":\n",
    "            real_Label.append(1)\n",
    "        else:\n",
    "            real_Label.append(0)\n",
    "    print((real_Label),(predictedLabel))\n",
    "    return v_measure_score(ground_Truth,list(Predicted_Label))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining entropy\n",
    "def entropy(s):\n",
    "    p, lns = Counter(s), float(len(s))\n",
    "    return -sum( count/lns * math.log(count/lns, 2) \n",
    "                for count in p.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates the ngram dictionary and returns it\n",
    "def createTensor(file,ngram):\n",
    "    #create the ngrams first and then add them in the dictionary \n",
    "    nGram_dictionary = {}\n",
    "    for i in range(0,len(file),ngram):\n",
    "        ngram_word=file[i:i+ngram]\n",
    "        if ngram_word not in global_Dictionary_ngrams:\n",
    "            global_Dictionary_ngrams[ngram_word]=1\n",
    "        if ngram_word in nGram_dictionary:\n",
    "            nGram_dictionary[ngram_word][0]+=1\n",
    "            nGram_dictionary[ngram_word][1].append(location_Calculator(i,len(file)))\n",
    "        else:\n",
    "            nGram_dictionary[ngram_word]=[]\n",
    "            nGram_dictionary[ngram_word].append(1)\n",
    "            loc = location_Calculator(i,len(file))\n",
    "            nGram_dictionary[ngram_word].append([loc])  \n",
    "    return nGram_dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#given the file length and its ngram it returns the location in file\n",
    "def location_Calculator(i,length):\n",
    "    location_word = i/length*100\n",
    "    #print(location_word)\n",
    "    if location_word <=10:\n",
    "        return 10\n",
    "    elif location_word <=20:\n",
    "        return 20\n",
    "    elif location_word <=30:\n",
    "        return 30\n",
    "    elif location_word <=40:\n",
    "        return 40\n",
    "    elif location_word <=50:\n",
    "        return 50\n",
    "    elif location_word <=60:\n",
    "        return 60\n",
    "    elif location_word <=70:\n",
    "        return 70\n",
    "    elif location_word <=80:\n",
    "        return 80\n",
    "    elif location_word <=90:\n",
    "        return 90\n",
    "    return 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates the tensor from the given dictionaries\n",
    "def makeATensor(globalDict,fileDict):\n",
    "    #print(globalDict,fileDict)\n",
    "    tensor = np.zeros((numberOfFiles,len(globalDict),11))\n",
    "    count_file = 0\n",
    "    total_Ngrams = 0 \n",
    "    for f in text_files:\n",
    "        theFile = open(dataDir+f, \"rb\").read()\n",
    "        theFile = theFile[:1000]\n",
    "        for i in range(0,len(theFile),5):\n",
    "            total_Ngrams+=1\n",
    "            ngram_word=theFile[i:i+5]\n",
    "            index_File = count_file\n",
    "            index_Ngram = globalDict[ngram_word]\n",
    "            loc_Ngram = location_Calculator(i,len(theFile))\n",
    "            #print(index_File,index_Ngram,int(loc_Ngram/10))\n",
    "            tensor[index_File][index_Ngram][int(loc_Ngram/10)]+=1    \n",
    "        count_file +=1\n",
    "    file = open(\"tensorTesting.txt\",\"w\") \n",
    "    file.write(str(tensor))\n",
    "    file.close() \n",
    "#     print(tensor[1])\n",
    "#     #x = np.arange(200).reshape((4,5,10))\n",
    "#     with file('test.txt', 'w') as outfile:\n",
    "#         for slice_2d in tensor:\n",
    "#             np.savetxt(outfile, slice_2d)\n",
    "    print(\"Ngrams are\")\n",
    "    print(total_Ngrams)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#decomposition Tecnique for the given tensor\n",
    "def parafac_decomposition(tensor):\n",
    "        \"\"\"\n",
    "        Computes a parafac decomposition of the tensor.\n",
    "        This will return n rank 3 factor matrices, where n represents the \n",
    "        dimensionality of the tensor.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        decompose = KruskalTensor(tensor.shape, rank=3, regularize=1e-6, \n",
    "                                  init='nvecs', X_data=tensor)\n",
    "        factors = decompose.U\n",
    "        with tf.Session() as sess:\n",
    "            for i in range(len(factors)):\n",
    "                sess.run(factors[i].initializer)\n",
    "                factors[i] = factors[i].eval()\n",
    "        return factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for vizualizing and its clustering we use this function \n",
    "def k_means_clustering(factor_matrix):\n",
    "        clusters = 2\n",
    "        svd = TruncatedSVD(n_components=2, n_iter=20, random_state=42)\n",
    "        reduced = svd.fit_transform(factor_matrix)\n",
    "        kmeans = KMeans(n_clusters=clusters, random_state=0).fit(factor_matrix)\n",
    "        labels_predicted = kmeans.labels_\n",
    "        #print(labels_predicted)\n",
    "        #print(text_files)\n",
    "        #lets write the function to check the accuracy here \n",
    "        #we have 4 different categories 0-zeus 0-benign 1-zeus 0-benign\n",
    "        zero_Zeus = 0\n",
    "        zero_Benign = 0\n",
    "        one_Zeus = 0 \n",
    "        one_Benign = 0 \n",
    "        for i in range(0,len(text_files)):\n",
    "            #print(text_files[i][len(text_files[i])-3:],labels_predicted[i])\n",
    "            if text_files[i][len(text_files[i])-3:]==\"exe\" and labels_predicted[i]==0:\n",
    "                zero_Benign+=1\n",
    "            elif text_files[i][len(text_files[i])-3:]==\"exe\" and labels_predicted[i]==1:\n",
    "                one_Benign+=1\n",
    "            elif text_files[i][len(text_files[i])-3:]==\"ex0\" and labels_predicted[i]==0:\n",
    "                zero_Zeus+=1\n",
    "            elif text_files[i][len(text_files[i])-3:]==\"ex0\" and labels_predicted[i]==1:\n",
    "                one_Zeus+=1\n",
    "            else:\n",
    "                continue\n",
    "        print(zero_Zeus,zero_Benign,one_Zeus,one_Benign) \n",
    "        data = [plotly.graph_objs.Scatter(x=[entry[0] for entry in reduced],\n",
    "                                          y=[entry[1] for entry in reduced],\n",
    "                                          mode='markers',\n",
    "                                          marker=dict(color=kmeans.labels_),\n",
    "                                          text = text_files\n",
    "                                          )\n",
    "                ]\n",
    "        fig = go.Figure(data=data)\n",
    "        plotly.offline.plot(fig, filename='kmeans_cluster.html')\n",
    "        return text_files,labels_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to create the similarity matrix for the heatmap used\n",
    "def generate_cosine_similarity_matrix(matrix):\n",
    "    cosine_sim = []\n",
    "    for entry in matrix:\n",
    "        sim = []\n",
    "        for other_entry in matrix:\n",
    "            sim.append(spatial.distance.cosine(entry, other_entry) * -1 + 1)\n",
    "        cosine_sim.append(sim)\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to create the heatmap of the following tensor decomposition\n",
    "def generate_heat_map(data, axis_labels):\n",
    "    \"\"\"\n",
    "    Generates a heat map for the current data\n",
    "    Currently only meant to support using a cosine similarity matrix\n",
    "    :param data:\n",
    "    :param axis_labels:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    axis_labels_abbreviated = [label[:14] for label in axis_labels]\n",
    "    info = [go.Heatmap(z=data,\n",
    "                       x=axis_labels_abbreviated,\n",
    "                       y=axis_labels_abbreviated,\n",
    "                       colorscale='Hot',\n",
    "                       )]\n",
    "\n",
    "    layout = go.Layout(title='Cosine Similarity Between Documents',\n",
    "                       xaxis=dict(ticks=''),\n",
    "                       yaxis=dict(ticks=''),\n",
    "                       plot_bgcolor='#444',\n",
    "                       paper_bgcolor='#eee'\n",
    "                       )\n",
    "    fig = go.Figure(data=info, layout=layout)\n",
    "    plotly.offline.plot(fig, filename='heatmap.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngrams are\n",
      "37456\n",
      "0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "#This the clock where we run the whole program and its the driver program!\n",
    "for f in text_files:\n",
    "    count+=1\n",
    "    if count == 250:\n",
    "        print(\"read file \",f)\n",
    "        count = 0\n",
    "    theFile = open(dataDir+f, \"rb\").read()\n",
    "    #print(len(theFile))\n",
    "    #call the tensor decomposition stuff here and create the tensor here\n",
    "    dictNgrams_file = createTensor(theFile[:1000],5)\n",
    "    file_Dictionary_ngram.append(dictNgrams_file)\n",
    "    #print(dictNgrams_file)\n",
    "    length_Ngram_Dictionaries.append(len(dictNgrams_file))\n",
    "    for key in dictNgrams_file:\n",
    "        if dictNgrams_file[key][0]>1:\n",
    "            mostFrequentNgrams.append([key,dictNgrams_file[key]])\n",
    "    #print(mostFrequentNgrams)\n",
    "    #print(\"Next Exe File\")\n",
    "#print(max(length_Ngram_Dictionaries))\n",
    "j = 0\n",
    "for key in global_Dictionary_ngrams:\n",
    "    global_Dictionary_ngrams[key]=j\n",
    "    j = j+1    \n",
    "#call a function which takes the global dictionary, each file dictionary and creates the tensor\n",
    "tensor_Ngram = makeATensor(global_Dictionary_ngrams,file_Dictionary_ngram)\n",
    "#print(global_Dictionary_ngrams)   \n",
    "#print(tensor_Ngram)\n",
    "#print(np.amax(tensor_Ngram))    \n",
    "#print(np.sum(tensor_Ngram))\n",
    "#Got the tensor here as tensor_Ngram, now lets form its factor matrices \n",
    "#and then try to cluster them \n",
    "factors_Ngrams = parafac_decomposition(tensor_Ngram)\n",
    "#print(factors_Ngrams[2])\n",
    "#lets send the factor matrices to k means clustering to see how the files will be clustered\n",
    "\n",
    "k_means_clustering(factors_Ngrams[0])\n",
    "#here we are done with the ploting \n",
    "# let's check how correct we are goin in the current process\n",
    "#Run next cell for the accuracies\n",
    "cos_sim = generate_cosine_similarity_matrix(factors_Ngrams[0])\n",
    "generate_heat_map(cos_sim, text_files)\n",
    "\n",
    "# if args['heatmap']:\n",
    "#         cos_sim = tdt.generate_cosine_similarity_matrix(factors[args['axis']])\n",
    "#         visualize.generate_heat_map(cos_sim, tdt.corpus_names)\n",
    "#     if args['kmeans']:\n",
    "#         visualize.k_means_clustering(factors[args['axis']], tdt.corpus_names, \n",
    "#                                      clusters=args['components'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
